#!/bin/bash
#SBATCH -J md
#SBATCH -p quick
#SBATCH --time=12:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=64
#SBATCH --gres=gpu:4

### Specify number of replicas/windows here ###
### Make sure: 1. number of replicas/windows equals the number in ppn=<nproc>
###            2. number of replicas/windows is divisible by the number in gpus=<ngpu>

NUMBER_OF_WINDOWS=32

### Specify number of replicas/windows here ###

echo "Start time: $(date)"
echo "SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST"
echo "hostname: $(hostname)" 
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
echo "Job directory: $(pwd)"

export OMP_NUM_THREADS=2

# Decide the software version
source /public/home/yqyang/software/gmx_2203_plumed-installed_1/bin/GMXRC.bash
export LD_LIBRARY_PATH=/public/software/lib/:$LD_LIBRARY_PATH
source /public/software/compiler/intel/intel-compiler-2017.5.239/bin/compilervars.sh intel64
export PATH="/public/home/yqyang/software/mpich-4.1.1-installed/bin:"$PATH
export PATH="/public/home/yqyang/software/plumed-2.8.1-installed_mpi_1/bin:"$PATH
export LD_LIBRARY_PATH="/public/home/yqyang/software/mpich-4.1.1-installed/lib:"$LD_LIBRARY_PATH
export LD_LIBRARY_PATH="/public/home/yqyang/software/plumed-2.8.1-installed_mpi_1/lib:"$LD_LIBRARY_PATH

# Define variables
num_windows=${NUMBER_OF_WINDOWS}
window_start=0
window_end=$((num_windows - 1))
directories=$(seq "$window_start" "$window_end" | awk '{printf "lambda%s ", $0}')
num_gpus=$(echo "$CUDA_VISIBLE_DEVICES" | tr ',' '\n' | wc -l)
num_gpus_minus_one=$((num_gpus - 1))
gpu_ids=$(seq 0 "$num_gpus_minus_one" | awk '{printf "%s", $0}')

# Energy Minimization
for window_idx in $(seq "$window_start" "$window_end"); do
  gmx_mpi grompp -f "lambda$window_idx/em.mdp" -c "../ions.gro" -r "../ions.gro" -p "../newtop.top" -o "lambda$window_idx/em.tpr" -n ../index.ndx -maxwarn 4
done
mpirun -np ${NUMBER_OF_WINDOWS} gmx_mpi mdrun -v -deffnm em -multidir `echo "${directories}"` -gpu_id "${gpu_ids}"

# NVT Equilibration
for window_idx in $(seq "$window_start" "$window_end"); do
  gmx_mpi grompp -f "lambda$window_idx/nvt.mdp" -c "lambda$window_idx/em.gro" -r "lambda$window_idx/em.gro" -p "../newtop.top" -o "lambda$window_idx/nvt.tpr" -n ../index.ndx -maxwarn 4
done
mpirun -np ${NUMBER_OF_WINDOWS} gmx_mpi mdrun -v -deffnm nvt -nb gpu -bonded gpu -multidir `echo "${directories}"` -gpu_id "${gpu_ids}"

for window_idx in $(seq "$window_start" "$window_end"); do
  gmx_mpi grompp -f "lambda$window_idx/npt.mdp" -c "lambda$window_idx/nvt.gro" -r "lambda$window_idx/nvt.gro" -p "../newtop.top" -o "lambda$window_idx/npt.tpr" -n ../index.ndx -maxwarn 4
done
mpirun -np ${NUMBER_OF_WINDOWS} gmx_mpi mdrun -v -deffnm npt -nb gpu -bonded gpu -multidir `echo "${directories}"` -gpu_id "${gpu_ids}"

# MD Simulation
for window_idx in $(seq "$window_start" "$window_end"); do
  gmx_mpi grompp -f "lambda$window_idx/md.mdp" -c "lambda$window_idx/npt.gro" -p "../newtop.top" -o "lambda$window_idx/md.tpr" -n ../index.ndx -maxwarn 4
done
mpirun -np ${NUMBER_OF_WINDOWS} gmx_mpi mdrun -v -deffnm md -nb gpu -bonded gpu -replex 1000 -multidir `echo "${directories}"` -gpu_id "${gpu_ids}" -pin on -dhdl dhdl

